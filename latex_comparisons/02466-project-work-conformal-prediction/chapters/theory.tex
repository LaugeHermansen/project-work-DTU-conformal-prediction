\section{Theory}
% David doing some explanations
Conformal prediction is an extension to a model used for either a classification or a regression task that include uncertainty measures for the prediction. %The reader can think of CP as a way of reshaping the confidence intervals/sets of the model to statistically rigorous ones. 

The model that CP is applied on needs to output some uncertainty measure, however this measure does not necessarily need to be true or accurate. The goal of CP is to change the uncertainty measure into a statistically rigorous one. 

CP does this in three steps, it defines a score function that maps the uncertainty outputs of the model to some score, then it computes the desired quantile of a calibration set and at last computes the same score on the new data point and selects all scores below this quantile as the prediction set (for regression this last step is slightly different). 

The score function is chosen such that the more certain the model is on the real output the smaller it becomes, hence the further form the truth the models prediction is the larger the score becomes. 

Next the scores are computed for a calibration set and the $\frac{\ceil{(n+1)(1 - \alpha)}}{n}$ quantile is chosen as $\hat{q}$. Here $n$ is the amount of data points in the calibration set and $\alpha$ is the chosen error rate. This choice will be explained later, but the reader can think of this as the desired $1 - \alpha$ quantile when $n$ is large. 

At last the score function of each label is computed for the new data point. All labels with scores below $\hat{q}$ are in the prediction set. 

With these three steps CP ensures coverage of the prediction set. This coverage means that $1 - \alpha$ amount of the time when this entire procedure is performed the true label is in the prediction set for one new sample. This does not mean that $1 - \alpha$ amount of label predictions done with one realization of the algorithm will contain the true label. To get an intuition for this the reader can imagine being unlucky with the calibration set such that the calibration set does not represent the underlying distribution accurately. Hence, the confidence intervals will be quite poor. However, this will only happen rarely. As will be explained later the accuracies of each realization of a calibration set will be beta distributed with an expectation of $1 - \alpha$. 

Next the algorithm will be explained mathematically. 
% notations and definitions
\subsection{Notation and definitions}
Conformal Prediction is a way of doing uncertainty quantification without distributional assumptions or model assumptions. The only assumption is that all data points are exchangeable.\\
%
The goal of CP is to create valid confidence bounds from an existing model. Mathematically formulated the aim is to create a function, $\Tau: \X \rightarrow T$, that maps a data point from the input space to a subset of the output space, such that
\begin{gather}
\doubleP(y\in\Tau(X)) \geq 1-\alpha, \quad \forall X\in\X.
\label{eq: coverage}
\end{gather}
Here, $X$ is a sample, $y$ is its corresponding true label, $\X$ is the input space, $\Y$ is the output space, and $T = \{t:t\subseteq \Y\}$ are all possible subsets of $\Y$. In the future we will refer to elements in $T$ as either prediction sets or prediction intervals depending on the task; classification or regression respectively. Finally $\alpha$ is a pre-specified error rate, i.e. the chance of the true label being outside the prediction set.
% Description of the method - The algorithm
\subsection{Description of the method}
\label{sec: description of method}
First a machine learning (ML) model,
$f: \X\rightarrow \Y$,
suitable for the given task is chosen.
Then a score function,
$s:\X\times\Y\rightarrow\doubleR$,
is specified.
The score function is a heuristic, but in order for CP to be useful it should grow with the difficulty of the input. Once the score function is chosen, split the data into 3 splits: A training set, $(X^{train}, y^{train})$, a calibration set, $(X^{cal}, y^{cal})$, and a validation set, $(X^{val}, y^{val})$. The training set is used to train the underlying ML model, $f$, e.g. a neural network.

The calibration set is used to train, or 'calibrate', the conformal predictor. It does so by computing the scores, $s_i = (X^{cal}_i, y^{cal}_i)$ for each data point in the calibration set. Here $(X^{cal}_i, y^{cal}_i)$ is the $i^{th}$ calibration point and its corresponding true label. Then let $\hat q$ be the $\ceil{(1-\alpha)(n+1)}/n$ quantile of the scores in the calibration set. Finally, $\Tau$ is defined such that 
\begin{gather}
\Tau(X) = \{y\in\Y : s(X,y) \leq \hat q\},
\label{eq: def. prediction set}
\end{gather}
where $X\in\X$ is one data point. In words, $\Tau(X)$ returns the possible $y\in\Y$ such that the score function of $(X,y)$ is less than or equal to $\hat q$. Note that $\hat q$ is an estimate of the true $1-\alpha$ quantile of the score distribution. Finally, the validation set is used to validate that \cref{eq: coverage} holds by estimating expectation below
\begin{gather}
    \doubleP(y\in\Tau(X)) = \doubleE\left[\doubleOne\left(y\in\Tau(X)\right)\right] \approx \frac{1}{n_{val}}\sum_{i=1}^{n_{val}}\left[\doubleOne\left(y^{val}_i\in\Tau(X^{val}_i)\right)\right]
    \label{eq: coverage expactation}
\end{gather}

% 
% Exchangeability 
%
\subsection{Exchangeability}
CP requires very few assumptions unlike many other probabilistic models. It requires that the data samples in the calibration and validation set are exchangeable. Here it is important to explain the distinction between exchangeable and i.i.d. data points. Exchangeable means that 
\[
P_{X, Y}(X=x, Y=y) = P_{X, Y}(X=y, Y=x)
\]
which might seem the same as i.i.d. as we state that x and y need to follow the same distribution. However, exchangeability allows $X$ and $Y$ to be correlated because of a shared confounder. This also means that conditionally on all confounders exchangeable random variables are independent 
\[
P_{X, Y}(X=x, Y=y|Z=z) = P_{X, Y}(X=y, Y=x|Z=z) = P_{X}(X=y|Z=z)P_{Y}(Y=x|Z=z)
\]
Here $Z$ describes all confounders for $X$ and $Y$ and they are therefore conditionally independent. 
%
% Proof of coverage.
%
\subsection{Proof of coverage}
Now follows the proof that the statement in \cref{eq: coverage} is actually true when applying CP. Let $s_i$ be the score function of the $i$'th data point in the calibration set, i.e., $s_i=s(X_i^{cal},y_i^{cal})$ for $i=1,2,\dots,n$, where $n$ is the number of data points in the calibration set. These will be referred to as calibration scores.
Now, let $s'$ be the score of a new data point, $X$, with true label, $y$. Since the scores are assumed exchangeable, the probability that the rank of $s'$ is at most $k$ is $k/(n+1)$. Here $rank(s')=k$ means that $s'$ is the $k$'th smallest element in the set. This is because the calibration scores together with $s'$ are $n+1$ exchangeable random variables, so the chance of either one of them having a specific rank for any realization is exactly $1/(n+1)$. As there are $k$ different ranks less than or equal to $k$, this yields
\begin{gather}
    \doubleP(rank(s')\leq k) = \frac{k}{n+1}
    \label{eq: prob k}
\end{gather}
% \begin{gather}
%     \doubleP(s'\leq s_k) = \frac{k}{n+1}
%     \label{eq: prob k}
% \end{gather}
% where the scores have been reordered such that $rank(s_k)=k$.\\
Due to the construction of $\Tau$, $y$ is in the prediction set of a data point $X$, if and only if the score function of $(X,y)$ is less than or equal to $\hat q$, i.e., 
\begin{gather*}
    y\in\Tau(X) \Leftrightarrow s' \leq \hat q
\end{gather*}
This means that
\begin{gather}
    \doubleP(y\in\Tau(X)) = \doubleP(s' \leq \hat q)
    \label{eq: pred set prob}
\end{gather}
By definition, $\hat q$ is the value of the calibration score with rank $\ceil{(n+1)(1-\alpha)}$. Therefore, making the substitution $k := \ceil{(n+1)(1-\alpha)}$ in \cref{eq: prob k}, has the result
\begin{align*}
    \doubleP(s'\leq \hat q)
    &= \doubleP\big(rank(s') \leq \ceil{(n+1)(1-\alpha)}\big)\\
    &= \frac{\ceil{(n+1)(1-\alpha)}}{n+1}
    \geq \frac{(n+1)(1-\alpha)}{n+1}\\
    &= 1-\alpha
\end{align*}
Finally, combining this with \cref{eq: pred set prob}, we arrive at
\begin{gather}
    \doubleP(y\in\Tau(X)) \geq 1-\alpha
\end{gather}
LOWER BOUND WILL FOLLOW?!
%
%
%
%
%
%
%
%
\subsection{Statistical evaluation of coverage}
The coverage will now be examined. First, let $C$ be the estimated coverage, and let $\mu$ be the true coverage as defined in \cref{eq: coverage expactation}.
%
\begin{gather}
\label{eq: coverage mu}
C = \frac{1}{n_{val}}\sum_{i=1}^{n_{val}}\left[\doubleOne\left(y^{val}_i\in\Tau(X^{val}_i)\right)\right]\\
\label{eq: coverage C}
\mu = \doubleE_{\X \times\Y}\left[\doubleOne\left(y\in\Tau(X)\right)\right]=\doubleE[C]
\end{gather}
%
The notation in the definition of $\mu$ means that it is an expectation over the entire input and output space. However, the function $\Tau$ depends on the calibration set, so for different calibration sets, the expectation, $\mu$, will have different values. Hence, the expectation is a random variable itself. This aligns well with the intuition that $\hat q$ is only an estimate of the true $1-\alpha$ quantile of the scores, so different calibration sets will have different coverage. One could imagine an unlucky realization of a calibration set that would yield a coverage lower than $1-\alpha$. However, this seems to contradict the fact that the lower bound of coverage was just proven to be $1-\alpha$.

The key is to look at \cref{eq: prob k}. It only holds because all the $s_i$'s and $s'$ are $n+1$ random variables that are realized simultaneously. If instead the $s_i$'s were already realized, the equality in \cref{eq: prob k} would not hold, because naturally, the probability of $s'$ having rank $k$ would depend on the realizations. Specifically, assuming the $s_i$'s are in sorted order, the probability is given by,
\begin{gather}
    \doubleP\left(rank(s')\leq k\right) = \int_{-\infty}^{s_{k}} p(s) \d s ,
\end{gather}
where $p$ is the probability density function for the scores, and $s_k$ is the $k$'th largest element of the realizations. Assume again that the $s_i$'s are random variables. These reflections show that, the coverage guarantee only holds for each conduction of the procedure - not for a specific realization of calibration set. Equivalently, the proof guarantees that the expectation of $\mu$ is greater than or equal to $1-\alpha$, but not every realization of $\mu$.

In fact, $\mu$ is a probability, and as usual with probabilities of probabilities, $\mu$ follows a beta distribution. Turning the attention to $C$ it appears to be a sum of indicator variables, so it is tempting to conclude that $C\cdot n_{val}$ follows a binomial distribution with parameters $(n_{val}, \mu)$. This is true when conditioning on a calibration set, because then $\mu$ is fixed. However, since $\mu$ follows a beta distribution, $C\cdot n_{val}$ actually follows what's called a BetaBinomial distribution. To sum up,
\begin{align}
    &C|(X^{cal},y^{cal}) \sim \frac 1{n_{val}} Binom\big(n_{val},\mu |(X^{cal},y^{cal})\big)\\
    &\mu \sim Beta(a, b)\\
    &C \sim \frac 1{n_{val}} BetaBinom(n_{val}, a, b)
\end{align}
----------------- Alternativt kunne man vælge at skrive sådan her: ------------------
\begin{gather}
    C \sim \frac 1{n_{val}} Binom\big(n_{val},\mu)\big),
    \quad \text{where} \quad
    \mu \sim Beta(a, b)
\end{gather}
which means that,
\begin{gather}
    C \sim \frac 1{n_{val}} BetaBinom(n_{val}, a, b)
\end{gather}
------------------------------ anyway - back again ----------------------------------------------

where $a = \ceil{(n+1)(1-\alpha)}$ and $b=\lfloor(n+1)\alpha\rfloor$. Now follows a short elaboration on why the parameters to the beta distribution make sense. In a beta distribution the parameters $a$ and $b$ can be thought of as the number of successes and failures respectively, and here we define a success as the true label being in the prediction set. Recall that the calibration set size is $n$ and $\hat q$ is defined as the $a$'th element in the calibration scores. Then it makes sense to say that the prior knowledge of $\mu$ is $a$ successes and $b$ failures, because in fact
\begin{gather}
    a+b=\ceil{(n+1)(1-\alpha)}
    + \lfloor(n+1) \alpha\rfloor = n+1,
\end{gather}
and we had exactly $n+1$ elements in the proof.

%
% Marginal vs. Conditional coverage 
%
\subsection{Coverage - marginal vs. conditional}
Coverage in itself is not impressive as a model with $100\%$ coverage can easily be made by returning the entire output/label space for each data point. This is not useful at all and it is therefore preferable to obtain coverage with the smallest prediction set possible (this will however depend a lot on the model because if that is not well suited for the task the prediction set from CP will be large). 

Next we distinguish between marginal coverage and conditional coverage. Marginal coverage entails that the procedure will classify a new data sample correctly $1 - \alpha$ amount of the time, however, it might not classify all groups equally well. An example could be a prediction task with where the model needs to find out if a patient has a disease or not. This disease is quite rare and only one out a hundred has it. This means that a model that just classifies all patients as not having the disease will obtain marginal coverage of $99\%$. 

A more useful property is conditional coverage which state that the model will obtain coverage within each group. In the previous example the model would need to at least correctly classify $1-\alpha$ of the sick people as sick and $1 - \alpha$ of healthy people as healthy.